{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **importing** **libraries**"
      ],
      "metadata": {
        "id": "SpT5nHM760uI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kRcAxso95ce"
      },
      "outputs": [],
      "source": [
        "# Core\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Visualization (optional)\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# NLP\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "# ML / DL\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "\n",
        "# TensorFlow / Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.optimizers import Adam\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load** **data**"
      ],
      "metadata": {
        "id": "0sx_hB_j6y1_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(\"/content/train.csv\")\n",
        "test_df  = pd.read_csv(\"/content/test.csv\")\n",
        "\n",
        "label_cols = [\n",
        "    \"toxic\",\n",
        "    \"severe_toxic\",\n",
        "    \"obscene\",\n",
        "    \"threat\",\n",
        "    \"insult\",\n",
        "    \"identity_hate\"\n",
        "]\n",
        "\n",
        "train_df.head()"
      ],
      "metadata": {
        "id": "dv5G6mH7-6ND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# clean the text"
      ],
      "metadata": {
        "id": "Sowu8MsA8l0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
        "    words = text.split()\n",
        "    return \" \".join([w for w in words if w not in stop_words])\n",
        "\n",
        "train_df[\"cleaned_comment\"] = train_df[\"comment_text\"].apply(clean_text)\n",
        "test_df[\"cleaned_comment\"]  = test_df[\"comment_text\"].apply(clean_text)\n",
        "\n",
        "train_df[[\"comment_text\", \"cleaned_comment\"]].head()"
      ],
      "metadata": {
        "id": "Ob4BM1IUUoEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# tokenizing and padding"
      ],
      "metadata": {
        "id": "JB-87lzu8_Rf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words=20000)\n",
        "tokenizer.fit_on_texts(train_df[\"cleaned_comment\"])\n",
        "\n",
        "X = pad_sequences(\n",
        "    tokenizer.texts_to_sequences(train_df[\"cleaned_comment\"]),\n",
        "    maxlen=200,\n",
        "    padding=\"post\"\n",
        ")\n",
        "\n",
        "y = train_df[label_cols].values"
      ],
      "metadata": {
        "id": "jhmrMqTOIATX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# data spliting into train and validation"
      ],
      "metadata": {
        "id": "jgXnN-UO9QPg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_tr, X_val, y_tr, y_val = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n"
      ],
      "metadata": {
        "id": "M-qXQn6iIPHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# experiment with cnn,lstm and bert models"
      ],
      "metadata": {
        "id": "UGLrDi029fZO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1st model with CNN"
      ],
      "metadata": {
        "id": "Cnj7KIVm9vCN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_model = Sequential([\n",
        "    Embedding(20000, 128),\n",
        "    Conv1D(128, 5, activation=\"relu\"),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(6, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "cnn_model.compile(\n",
        "    optimizer=Adam(0.001),\n",
        "    loss=\"binary_crossentropy\"\n",
        ")\n",
        "\n",
        "cnn_model.fit(\n",
        "    X_tr, y_tr,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=5,\n",
        "    batch_size=64\n",
        ")"
      ],
      "metadata": {
        "id": "YPQaBa6bIVz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2nd model with LSTM"
      ],
      "metadata": {
        "id": "LH8QF6zl-BT3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_model = Sequential([\n",
        "    Embedding(20000, 128),\n",
        "    LSTM(64),\n",
        "    Dropout(0.5),\n",
        "    Dense(6, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "lstm_model.compile(\n",
        "    optimizer=Adam(0.001),\n",
        "    loss=\"binary_crossentropy\"\n",
        ")\n",
        "\n",
        "lstm_model.fit(\n",
        "    X_tr, y_tr,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=5,\n",
        "    batch_size=64\n",
        ")"
      ],
      "metadata": {
        "id": "YRGn5B98IdLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN and LSTM model evaluation"
      ],
      "metadata": {
        "id": "aIEZYU0C-LO2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, X_val, y_val, name):\n",
        "    print(f\"\\n===== {name} Evaluation =====\")\n",
        "    y_pred = (model.predict(X_val) > 0.5).astype(int)\n",
        "\n",
        "    print(classification_report(\n",
        "        y_val,\n",
        "        y_pred,\n",
        "        target_names=label_cols,\n",
        "        zero_division=0\n",
        "    ))\n",
        "\n",
        "    macro_f1 = f1_score(y_val, y_pred, average=\"macro\")\n",
        "    print(\"Macro F1-score:\", macro_f1)"
      ],
      "metadata": {
        "id": "Jq9HWQoTIpt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(cnn_model, X_val, y_val, \"CNN\")\n",
        "evaluate_model(lstm_model, X_val, y_val, \"LSTM\")\n"
      ],
      "metadata": {
        "id": "cunu3YMwIvz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Transformers Dataset"
      ],
      "metadata": {
        "id": "cUbjXoIBkPql"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets\n"
      ],
      "metadata": {
        "id": "5Ccahs5gZ_7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y transformers accelerate peft\n",
        "!pip install transformers==4.38.2 accelerate==0.27.2 peft==0.9.0\n"
      ],
      "metadata": {
        "id": "X9dwmqxdbZFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DistilBert model"
      ],
      "metadata": {
        "id": "DqrFnqcYqkAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from datasets import Dataset\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "\n",
        "from transformers import (\n",
        "    DistilBertTokenizerFast,\n",
        "    DistilBertForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")"
      ],
      "metadata": {
        "id": "TLxvhImEBpJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare Dataset"
      ],
      "metadata": {
        "id": "hdcnr8i-fkJR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(\"/content/train.csv\")\n",
        "\n",
        "label_cols = [\n",
        "    \"toxic\",\n",
        "    \"severe_toxic\",\n",
        "    \"obscene\",\n",
        "    \"threat\",\n",
        "    \"insult\",\n",
        "    \"identity_hate\"\n",
        "]\n",
        "\n",
        "train_df = train_df[[\"comment_text\"] + label_cols]\n"
      ],
      "metadata": {
        "id": "uwCWnIquqqIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = Dataset.from_pandas(train_df)\n",
        "dataset = dataset.train_test_split(test_size=0.2, seed=42)\n"
      ],
      "metadata": {
        "id": "V29sLXgPqwy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "ux3F8w2efnY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = DistilBertTokenizerFast.from_pretrained(\n",
        "    \"distilbert-base-uncased\"\n",
        ")\n",
        "\n",
        "def tokenize(batch):\n",
        "    return tokenizer(\n",
        "        batch[\"comment_text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=64\n",
        "    )\n",
        "\n",
        "dataset = dataset.map(tokenize, batched=True)\n"
      ],
      "metadata": {
        "id": "cvy_ae3WqxZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Add Labels"
      ],
      "metadata": {
        "id": "2xP-E-FtfxP5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_labels(batch):\n",
        "    batch[\"labels\"] = [\n",
        "        [float(batch[col][i]) for col in label_cols]\n",
        "        for i in range(len(batch[label_cols[0]]))\n",
        "    ]\n",
        "    return batch\n",
        "\n",
        "dataset = dataset.map(add_labels, batched=True)\n"
      ],
      "metadata": {
        "id": "8wpfumnbq20r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.set_format(\n",
        "    type=\"torch\",\n",
        "    columns=[\"input_ids\", \"attention_mask\", \"labels\"]\n",
        ")\n"
      ],
      "metadata": {
        "id": "OCwc-aXEq3nb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model & Training"
      ],
      "metadata": {
        "id": "fd5pynOzf8JD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = DistilBertForSequenceClassification.from_pretrained(\n",
        "    \"distilbert-base-uncased\",\n",
        "    num_labels=len(label_cols),\n",
        "    problem_type=\"multi_label_classification\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "_5Ir0TDkq9IE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./toxicity_results\",\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    fp16=True,\n",
        "    logging_steps=500,\n",
        "    save_strategy=\"no\",\n",
        "    report_to=\"none\"\n",
        ")"
      ],
      "metadata": {
        "id": "FrstdlTprCIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"test\"],\n",
        "    tokenizer=tokenizer\n",
        ")\n"
      ],
      "metadata": {
        "id": "ofuEaWQGrDFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "rVWl-oQTufPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation of DistilBert\n"
      ],
      "metadata": {
        "id": "-10cZ-sFqnRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, f1_score\n",
        "import torch\n",
        "\n",
        "preds = trainer.predict(dataset[\"test\"])\n",
        "\n",
        "y_true = preds.label_ids\n",
        "y_prob = torch.sigmoid(torch.tensor(preds.predictions)).numpy()\n",
        "y_pred = (y_prob > 0.5).astype(int)\n",
        "\n",
        "print(classification_report(\n",
        "    y_true,\n",
        "    y_pred,\n",
        "    target_names=label_cols,\n",
        "    zero_division=0\n",
        "))\n",
        "\n",
        "print(\"Macro F1-score:\",\n",
        "      f1_score(y_true, y_pred, average=\"macro\"))\n"
      ],
      "metadata": {
        "id": "k3PpRClizoLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save the Final Model"
      ],
      "metadata": {
        "id": "ptb2Y7HfgGPR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"toxicity_distilbert\")\n",
        "tokenizer.save_pretrained(\"toxicity_distilbert\")"
      ],
      "metadata": {
        "id": "IvtTx7qgJVMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# testing prediction"
      ],
      "metadata": {
        "id": "Kgtr-fuT_jOZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
        "\n",
        "label_cols = [\n",
        "    \"toxic\", \"severe_toxic\", \"obscene\",\n",
        "    \"threat\", \"insult\", \"identity_hate\"\n",
        "]\n",
        "\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained(\"toxicity_distilbert\")\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\"toxicity_distilbert\")\n",
        "model.eval()\n"
      ],
      "metadata": {
        "id": "rH6i2cDl6PEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_toxicity(text, threshold=0.5):\n",
        "    inputs = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=64\n",
        "    )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        probs = torch.sigmoid(outputs.logits).squeeze().numpy()\n",
        "\n",
        "    results = {\n",
        "        label: float(prob)\n",
        "        for label, prob in zip(label_cols, probs)\n",
        "    }\n",
        "\n",
        "    predictions = {\n",
        "        label: int(prob >= threshold)\n",
        "        for label, prob in results.items()\n",
        "    }\n",
        "\n",
        "    return results, predictions\n"
      ],
      "metadata": {
        "id": "S9ywI-RS6VSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Give Input and get Output"
      ],
      "metadata": {
        "id": "DUD8llQ0gZj-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"You are a disgusting idiot and should be banned\"\n",
        "scores, preds = predict_toxicity(text)\n",
        "\n",
        "scores, preds\n"
      ],
      "metadata": {
        "id": "A1hbutOV6aTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download the Model"
      ],
      "metadata": {
        "id": "_CpyRjAbgkJI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r toxicity_distilbert.zip toxicity_distilbert\n"
      ],
      "metadata": {
        "id": "mUqZYjFNKyMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"toxicity_distilbert.zip\")\n"
      ],
      "metadata": {
        "id": "42s3qqQQLIr5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}